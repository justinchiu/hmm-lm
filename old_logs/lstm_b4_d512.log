main.py --devid 2 --model-config configs/lstm-d512.yaml --num-epochs 100 --patience 6
{'--bptt': '35',
 '--bsz': '32',
 '--clip': '5',
 '--decay': '4',
 '--devid': '2',
 '--eval-bsz': '1',
 '--lr': '1e-3',
 '--model-config': 'configs/lstm-d512.yaml',
 '--nosave': False,
 '--num-checks': '4',
 '--num-epochs': '100',
 '--overfit': False,
 '--patience': '6',
 '--seed': '1111'}
LstmLm(
  (emb): Embedding(10001, 512, padding_idx=1)
  (lstm): LSTM(512, 512, num_layers=2, batch_first=True, dropout=0.5)
  (dropout): Dropout(p=0.5, inplace=False)
)
Num params, trainable: 9323008, 9323008
Valid eval: log_prob = -444540.84 | xent(word) = 6.03 | ppl = 414.41 | total_time = 0.41s 
Valid eval: log_prob = -429657.81 | xent(word) = 5.83 | ppl = 338.69 | total_time = 0.42s 
Valid eval: log_prob = -420312.16 | xent(word) = 5.70 | ppl = 298.38 | total_time = 0.41s 
Train epoch 0: log_prob = -5979750.00 | xent(word) = 6.43 | ppl = 621.84 | total_time = 33.11s 
Valid epoch 0: log_prob = -420293.84 | xent(word) = 5.70 | ppl = 298.31 | total_time = 0.42s 
Saving model to checkpoints/lstm-d512/0_5.70.pth
Valid eval: log_prob = -413403.53 | xent(word) = 5.60 | ppl = 271.70 | total_time = 0.43s 
Valid eval: log_prob = -408546.34 | xent(word) = 5.54 | ppl = 254.39 | total_time = 0.44s 
Valid eval: log_prob = -403691.28 | xent(word) = 5.47 | ppl = 238.18 | total_time = 0.53s 
Train epoch 1: log_prob = -5480600.50 | xent(word) = 5.90 | ppl = 363.48 | total_time = 35.04s 
Valid epoch 1: log_prob = -403656.94 | xent(word) = 5.47 | ppl = 238.07 | total_time = 0.50s 
Saving model to checkpoints/lstm-d512/1_5.47.pth
Valid eval: log_prob = -399821.16 | xent(word) = 5.42 | ppl = 226.01 | total_time = 0.45s 
