main.py --devid 0 --model-config configs/poehmm-d256-n3-k256-oldres.yaml --num-epochs 100 --bsz 4 --patience 8 --save poehmm_b4_d256_n3_k256_oldres
{'--bptt': '35',
 '--bsz': '4',
 '--clip': '5',
 '--decay': '4',
 '--devid': '0',
 '--eval-bsz': '1',
 '--lr': '1e-3',
 '--model-config': 'configs/poehmm-d256-n3-k256-oldres.yaml',
 '--num-checks': '4',
 '--num-epochs': '100',
 '--overfit': False,
 '--patience': '8',
 '--report-every': '5000',
 '--save': 'poehmm_b4_d256_n3_k256_oldres',
 '--seed': '1111'}
PoeHmmLm(
  (start_mlp): Sequential(
    (0): ResidualLayerOld(
      (lin1): Linear(in_features=256, out_features=256, bias=True)
      (lin2): Linear(in_features=256, out_features=256, bias=True)
      (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (1): Linear(in_features=256, out_features=1, bias=True)
  )
  (trans_mlp): Sequential(
    (0): ResidualLayerOld(
      (lin1): Linear(in_features=256, out_features=256, bias=True)
      (lin2): Linear(in_features=256, out_features=256, bias=True)
      (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (1): Linear(in_features=256, out_features=256, bias=True)
  )
  (terminal_mlp): Sequential(
    (0): ResidualLayerOld(
      (lin1): Linear(in_features=256, out_features=256, bias=True)
      (lin2): Linear(in_features=256, out_features=256, bias=True)
      (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (1): Linear(in_features=256, out_features=10001, bias=True)
  )
  (ff): FfEmission(
    (emb): Embedding(10003, 256, padding_idx=1)
    (cnn): Conv1d(256, 256, kernel_size=(2,), stride=(1,))
    (dropout): Dropout(p=0.3, inplace=False)
    (mlp): Sequential(
      (0): Linear(in_features=256, out_features=256, bias=True)
      (1): ReLU()
      (2): Dropout(p=0.3, inplace=False)
    )
    (proj): Linear(in_features=256, out_features=10001, bias=False)
  )
)
Num params, trainable: 8547346, 8547346
Traceback (most recent call last):

  File "main.py", line 198, in <module>
    main()

  File "main.py", line 172, in main
    verbose = True,

  File "main.py", line 70, in _loop
    losses = model.score(batch.text, mask=mask, lengths=lengths)

  File "/n/home13/jchiu/python/hmm-lm/models/poehmmlm.py", line 121, in score
    return self.ts_score(text, mask, lengths)

  File "/n/home13/jchiu/python/hmm-lm/models/poehmmlm.py", line 112, in ts_score
    elbo = ts.LinearChain(ts.MaxSemiring).sum(log_potentials, lengths=lengths).sum()

  File "/n/home13/jchiu/python/pytorch-struct/torch_struct/helpers.py", line 117, in sum
    v = self._dp(edge, lengths)[0]

  File "/n/home13/jchiu/python/pytorch-struct/torch_struct/linearchain.py", line 48, in _dp
    return self._dp_scan(log_potentials, lengths, force_grad)

  File "/n/home13/jchiu/python/pytorch-struct/torch_struct/linearchain.py", line 91, in _dp_scan
    chart = semiring.matmul(chart[:, :, 1::2], chart[:, :, 0::2])

  File "/n/home13/jchiu/python/pytorch-struct/torch_struct/semirings/semirings.py", line 190, in matmul
    return matmul(cls, a, b)

  File "/n/home13/jchiu/python/pytorch-struct/torch_struct/semirings/semirings.py", line 18, in matmul
    c = cls.sum(c.transpose(-2, -1))

  File "/n/home13/jchiu/python/pytorch-struct/torch_struct/semirings/semirings.py", line 194, in sum
    return torch.max(xs, dim=dim)[0]

RuntimeError: CUDA out of memory. Tried to allocate 4.00 GiB (GPU 0; 11.91 GiB total capacity; 6.68 GiB already allocated; 3.63 GiB free; 1.05 GiB cached)

