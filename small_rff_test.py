# -*- coding: utf-8 -*-
"""kernelhmm.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1X5UDkWNjUn3azgvToDjQim_C2fZff0pX
"""

import math

import torch
import torch_struct
import numpy as np
from genbmm import logbmm

N = 3 # batch size
T = 8 # length of sequence
V = 128 # vocab size

C = 64 # number of classes
H = 16 # embedding dimension
D = 5 # number of samples / projection dim

start_emb = torch.randn(H)
state_emb = torch.randn(C, H)
next_state_emb = torch.randn(C, H)
preterminal_emb = torch.randn(C, H)
terminal_emb = torch.randn(V, H)

state_emb.requires_grad = True
next_state_emb.requires_grad = True

projection = torch.randn(H, D)
text = torch.from_numpy(np.random.choice(V, size=(N, T)))
lengths = torch.from_numpy(np.random.choice(np.arange(T-3, T), size=(N,)))
lengths[0] = T
mask = torch.arange(T)[None] < lengths[:,None]
# no masking for now
print(text)
print(lengths)

# TODO: copy paste this for grad checking
log_phi_start = start_emb @ projection
log_phi_w = state_emb @ projection
log_phi_u = next_state_emb @ projection

start = (log_phi_start[None,None] + log_phi_u[None,:]).logsumexp(-1).log_softmax(-1)
transition = (log_phi_w[:,None] + log_phi_u[None,:]).logsumexp(-1).log_softmax(-1)
emission = (preterminal_emb @ terminal_emb.T).log_softmax(-1)

# TORCH STRUCT
log_potentials = torch_struct.LinearChain.hmm(
    transition = transition.T,
    emission = emission.T,
    init = start,
    observations = text,
)
evidence = torch_struct.LinearChain().sum(log_potentials, lengths=lengths)
# TODO: check grads

# LOOP VERSION
# gather emission
# N x T x C
logp_emit = emission[
    torch.arange(C)[None,None],
    text[:,:,None],
]
log_pots = logp_emit[:,1:,None,:] + transition[None,None]
log_eye = torch.empty(C, C).fill_(float("-inf"))
log_eye.diagonal().fill_(0.)
log_pots = torch.where(mask[:,1:,None,None], log_pots, log_eye[None,None])
alpha = start + logp_emit[:,0] # {N} x C
for t in range(T-1):
    # logbmm
    alpha = (alpha[:,:,None] + log_pots[:,t]).logsumexp(-2)
evidence_slow = alpha.logsumexp(-1)

# TODO: check grads

"""# Linear time implementation

We want to compute the following:
\begin{equation}
\begin{aligned}
p(x)
&= \sum_{z_1} p(z_1 \mid z_0)p(x_1 \mid z_1)  \sum_{z_2} p(z_2 \mid z_1)p(x_2 \mid z_2)\\          
&= \sum_{z_1} \frac{\phi(\mathbf{w}_{z_0})^T\phi(\mathbf{u}_{z_1})}{\sum_{z_1'}\phi(\mathbf{w}_{z_0})^T\phi(\mathbf{u}_{z'_1})}
p(x_1 \mid z_1)
\sum_{z_2} \frac{\phi(\mathbf{w}_{z_1})^T\phi(\mathbf{u}_{z_2})}{\sum_{z_2'}\phi(\mathbf{w}_{z_1})^T\phi(\mathbf{u}_{z_2'})}   
p(x_2 \mid z_2)\\
&= \left(\frac{\phi(\mathbf{w}_{z_0})^T}{\phi(\mathbf{w}_{z_0})^T\sum_{z_1'}\phi(\mathbf{u}_{z'_1})}\right)             
\left(
\sum_{z_1}
\frac{p\left(x_1 \mid z_1\right)\phi(\mathbf{u}_{z_1})\phi(\mathbf{w}_{z_1})^T}                                  
{
    \phi(\mathbf{w}_{z_1})^T\sum_{z_2'}\phi(\mathbf{u}_{z_2'})
}
\right)
\left(\sum_{z_2}p\left(x_2 \mid z_2\right)\phi(\mathbf{u}_{z_2})
\right),
\end{aligned}
\end{equation}
where $\mathbf{w}_z,\mathbf{u}_z\in\mathbb{R}^n$ are embeddings for state $z$.                                           

The summations over the zs can be computed independently.

For a sequence of length $T$, we compute the middle term $T-1$ times.

This relies on the distributive property of scalar-vector multiplication and dot product.
"""

# dot product "distributive property", which has generalizations to tensor products
x = torch.randn(3)
y = torch.randn(5, 3)
a = (x * y).sum()
b = x.dot(y.sum(0))
print(a,b)

"""Start with a log-space version of the linear space+time (in C) implementation"""

# RFF LOG-SPACE VERSION
logp_emit = emission[
    torch.arange(C)[None,None],
    text[:,:,None],
]
log_phi_start = start_emb @ projection
log_phi_w = state_emb @ projection
log_phi_u = next_state_emb @ projection

# EFFICIENCY: anything involving C we want to checkpoint away / logbmm / tvm / custom bwd

"""$$\left(\frac{\phi(\mathbf{w}_{z_0})^T}{\phi(\mathbf{w}_{z_0})^T\sum_{z_1'}\phi(\mathbf{u}_{z'_1})}\right) \in \mathbb{R}^d$$"""

## First term
# D
log_sum_phi_u = log_phi_u.logsumexp(0)
# SCALAR, logdot
log_start_denominator = (log_phi_start + log_sum_phi_u).logsumexp(0)
# D
log_start_vec = log_phi_start - log_start_denominator

"""$$\left(
\sum_{z_1}
\frac{p\left(x_1 \mid z_1\right)\phi(\mathbf{u}_{z_1})\phi(\mathbf{w}_{z_1})^T}                                  
{
    \phi(\mathbf{w}_{z_1})^T\sum_{z_2'}\phi(\mathbf{u}_{z_2'})
}
\right) \in \mathbb{R}^{d \times d}$$
for each time step except the last
"""

## Middle terms
# C = C x D + D
log_denominator = (log_phi_w + log_sum_phi_u).logsumexp(-1)
# C x Du x {Dw} + C x {Du} x Dw
log_numerator = log_phi_u[:,:,None] + log_phi_w[:,None,:]
# C x Du x Dw
log_trans_mat = log_numerator - log_denominator[:,None,None]
# N x T x C x {Du} x {Dw} + {N} x {T} x C x Du x Dw
#log_potentials = (logp_emit[:,:-1,:,None,None] + log_trans_mat[None,None]).logsumexp(2)

# logbmm(N x a x b, N x b x c)
log_potentials = logbmm(
    logp_emit[:,:-1].contiguous().view(1, -1, C).cuda(), # N x T x C
    log_trans_mat.view(1, C, -1).cuda(), # C x D x D
).view(N, T-1, D, D).cpu()
# masking is wrong! has an extra term, the denominator and right side vector.


"""$$\left(\sum_{z_2}p\left(x_2 \mid z_2\right)\phi(\mathbf{u}_{z_2})
\right) \in \mathbb{R}^d$$
"""

## Last term
# N x D
log_end_vec = (
    logp_emit[:,-1,:,None] # N x (T=-1) x C x {D}
    + log_phi_u[None] # {N} x C x D
).logsumexp(1)
mask_t = mask[:,-1]
log_end_vec = log_end_vec.masked_fill(~mask_t[:,None], 0.)
"""Compute the sequence of matrix vector products"""

# can use tvm here
evidence0 = log_start_vec[None] # {N} x Dw
for t in range(T-1):
    # logbmm
    evidence0 = (evidence0[:,:,None] + log_potentials[:,t]).logsumexp(-2)
evidence0 = (evidence0 + log_end_vec).logsumexp(-1)

# TODO: check grads

"""the above for loop can be replaced with tvm"""

log_pots = log_potentials.clone()
log_pots[:,0] += log_start_vec[None,:,None]
log_pots[:,-1] += log_end_vec[:,None,:]
evidence1 = torch_struct.LinearChain().sum(log_pots.transpose(-1, -2))

"""correct masking"""
log_potentials0 = logbmm(
    logp_emit.view(1, -1, C).cuda(), # N x T x C
    log_trans_mat.view(1, C, -1).cuda(), # C x D x D
).view(N, T, D, D).cpu()
log_eye = torch.empty(D,D).fill_(float("-inf"))
log_eye.diagonal().fill_(0.)
log_potentials0 = torch.where(mask[:,:,None,None], log_potentials0, log_eye[None,None])

log_end_vec0 = (
    logp_emit[torch.arange(N),lengths-1,:,None] # N x C x {D}
    + log_phi_u[None] # {N} x C x D
).logsumexp(1)

log_potentials0[torch.arange(N), lengths-1,:,0] = log_end_vec0
log_potentials0[torch.arange(N), lengths-1,:,1:] = float("-inf")
#log_potentials0[torch.arange(N), lengths-1,:,:] = log_end_vec0[:,:,None]
#log_potentials0[torch.arange(N), lengths-1,1:,:] = float("-inf")
log_potentials0[:,0] += log_start_vec[None,:,None]
evidence2 = torch_struct.LinearChain().sum(log_potentials0.transpose(-1, -2))


#import pdb; pdb.set_trace()
print(evidence)
print(evidence_slow)
print(evidence0)
print(evidence1)
print(evidence2)
